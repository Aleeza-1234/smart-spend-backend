{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e7aa36",
   "metadata": {},
   "source": [
    "# SmartSpend Transaction Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on transaction data for the SmartSpend application. We'll analyze spending patterns, categorize transactions, detect anomalies, and build predictive models.\n",
    "\n",
    "## Overview\n",
    "- Data loading and preprocessing\n",
    "- Spending pattern visualization\n",
    "- Category analysis\n",
    "- Anomaly detection\n",
    "- Expense prediction modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca408b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f963e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f30c9a",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d899cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample transaction data\n",
    "df = pd.read_csv('../app/ml/data/sample_transactions.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['month'] = df['date'].dt.month_name()\n",
    "df['hour'] = df['date'].dt.hour if 'hour' in df['date'].dt else 12  # Default to noon if no time\n",
    "\n",
    "# Display data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2bfd6b",
   "metadata": {},
   "source": [
    "## 3. Spending Pattern Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8240ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spending visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Daily spending over time\n",
    "daily_spending = df.groupby('date')['amount'].sum()\n",
    "axes[0, 0].plot(daily_spending.index, daily_spending.values, marker='o')\n",
    "axes[0, 0].set_title('Daily Spending Over Time')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Amount ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Spending by category\n",
    "category_spending = df.groupby('category')['amount'].sum().sort_values(ascending=True)\n",
    "axes[0, 1].barh(category_spending.index, category_spending.values)\n",
    "axes[0, 1].set_title('Total Spending by Category')\n",
    "axes[0, 1].set_xlabel('Amount ($)')\n",
    "\n",
    "# 3. Spending by day of week\n",
    "dow_spending = df.groupby('day_of_week')['amount'].mean()\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_spending = dow_spending.reindex(dow_order)\n",
    "axes[1, 0].bar(range(len(dow_spending)), dow_spending.values)\n",
    "axes[1, 0].set_title('Average Spending by Day of Week')\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Average Amount ($)')\n",
    "axes[1, 0].set_xticks(range(len(dow_spending)))\n",
    "axes[1, 0].set_xticklabels(dow_spending.index, rotation=45)\n",
    "\n",
    "# 4. Transaction amount distribution\n",
    "axes[1, 1].hist(df['amount'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Transaction Amount Distribution')\n",
    "axes[1, 1].set_xlabel('Amount ($)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235474c2",
   "metadata": {},
   "source": [
    "## 4. Category Analysis with Interactive Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive pie chart for category distribution\n",
    "category_totals = df.groupby('category')['amount'].sum()\n",
    "\n",
    "fig = px.pie(values=category_totals.values, \n",
    "             names=category_totals.index,\n",
    "             title='Spending Distribution by Category')\n",
    "fig.show()\n",
    "\n",
    "# Interactive bar chart for merchant spending\n",
    "merchant_spending = df.groupby('merchant')['amount'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "fig = px.bar(x=merchant_spending.values, \n",
    "             y=merchant_spending.index,\n",
    "             orientation='h',\n",
    "             title='Top 10 Merchants by Spending',\n",
    "             labels={'x': 'Amount ($)', 'y': 'Merchant'})\n",
    "fig.show()\n",
    "\n",
    "# Category spending over time\n",
    "fig = px.line(df.groupby(['date', 'category'])['amount'].sum().reset_index(),\n",
    "              x='date', y='amount', color='category',\n",
    "              title='Category Spending Trends Over Time')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e05660b",
   "metadata": {},
   "source": [
    "## 5. Anomaly Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee4232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical anomaly detection using IQR method\n",
    "def detect_outliers_iqr(data):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return (data < lower_bound) | (data > upper_bound)\n",
    "\n",
    "# Detect amount-based outliers\n",
    "df['is_outlier'] = detect_outliers_iqr(df['amount'])\n",
    "outliers = df[df['is_outlier']]\n",
    "\n",
    "print(f\"Detected {len(outliers)} outlier transactions out of {len(df)} total transactions\")\n",
    "print(\"\\nOutlier transactions:\")\n",
    "print(outliers[['date', 'amount', 'description', 'category', 'merchant']])\n",
    "\n",
    "# Visualize outliers\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Box plot with outliers\n",
    "ax1.boxplot(df['amount'], vert=True)\n",
    "ax1.set_title('Transaction Amounts with Outliers')\n",
    "ax1.set_ylabel('Amount ($)')\n",
    "\n",
    "# Scatter plot highlighting outliers\n",
    "colors = ['red' if outlier else 'blue' for outlier in df['is_outlier']]\n",
    "ax2.scatter(range(len(df)), df['amount'], c=colors, alpha=0.6)\n",
    "ax2.set_title('Transaction Amounts (Red = Outliers)')\n",
    "ax2.set_xlabel('Transaction Index')\n",
    "ax2.set_ylabel('Amount ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning-based anomaly detection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Prepare features for anomaly detection\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "le_category = LabelEncoder()\n",
    "le_merchant = LabelEncoder()\n",
    "\n",
    "df_ml['category_encoded'] = le_category.fit_transform(df_ml['category'])\n",
    "df_ml['merchant_encoded'] = le_merchant.fit_transform(df_ml['merchant'])\n",
    "df_ml['day_of_week_encoded'] = df_ml['date'].dt.dayofweek\n",
    "\n",
    "# Select features for anomaly detection\n",
    "features = ['amount', 'category_encoded', 'merchant_encoded', 'day_of_week_encoded', 'user_id']\n",
    "X = df_ml[features]\n",
    "\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "df_ml['anomaly_score'] = iso_forest.fit_predict(X)\n",
    "df_ml['is_anomaly'] = df_ml['anomaly_score'] == -1\n",
    "\n",
    "ml_anomalies = df_ml[df_ml['is_anomaly']]\n",
    "\n",
    "print(f\"ML detected {len(ml_anomalies)} anomalous transactions\")\n",
    "print(\"\\nML-detected anomalies:\")\n",
    "print(ml_anomalies[['date', 'amount', 'description', 'category', 'merchant']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da36794",
   "metadata": {},
   "source": [
    "## 6. Category Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b412c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a text classification model for transaction categories\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Prepare text features (description + merchant)\n",
    "df['text_features'] = df['description'] + ' ' + df['merchant']\n",
    "\n",
    "# Vectorize text features\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english', lowercase=True)\n",
    "X_text = vectorizer.fit_transform(df['text_features'])\n",
    "\n",
    "# Target variable\n",
    "y = df['category']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Category Classification Accuracy: {accuracy:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9992bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "\n",
    "# Get top 15 most important features\n",
    "top_features_idx = np.argsort(feature_importance)[-15:]\n",
    "top_features = [feature_names[i] for i in top_features_idx]\n",
    "top_importance = feature_importance[top_features_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_features)), top_importance)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Most Important Features for Category Classification')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test the model with new examples\n",
    "test_descriptions = [\n",
    "    \"McDonald's Drive Thru\",\n",
    "    \"Uber ride to airport\", \n",
    "    \"Netflix monthly subscription\",\n",
    "    \"Target grocery shopping\"\n",
    "]\n",
    "\n",
    "print(\"\\nModel Predictions on New Examples:\")\n",
    "for desc in test_descriptions:\n",
    "    text_vec = vectorizer.transform([desc])\n",
    "    prediction = rf_classifier.predict(text_vec)[0]\n",
    "    probability = rf_classifier.predict_proba(text_vec)[0].max()\n",
    "    print(f\"'{desc}' -> {prediction} (confidence: {probability:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd01f68",
   "metadata": {},
   "source": [
    "## 7. Expense Prediction Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate additional synthetic data for expense prediction\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2024-01-01', end='2024-03-31', freq='D')\n",
    "synthetic_data = []\n",
    "\n",
    "for date in dates:\n",
    "    # Generate 1-5 transactions per day\n",
    "    num_transactions = np.random.randint(1, 6)\n",
    "    \n",
    "    for _ in range(num_transactions):\n",
    "        # Base amount with day-of-week and category variations\n",
    "        base_amount = np.random.normal(40, 15)\n",
    "        \n",
    "        # Weekend multiplier\n",
    "        if date.weekday() >= 5:\n",
    "            base_amount *= 1.3\n",
    "            \n",
    "        # Category-specific adjustments\n",
    "        categories = ['food', 'transport', 'entertainment', 'shopping', 'utilities']\n",
    "        category = np.random.choice(categories)\n",
    "        \n",
    "        category_multipliers = {\n",
    "            'food': 0.8, 'transport': 0.6, 'entertainment': 1.2, \n",
    "            'shopping': 1.5, 'utilities': 2.0\n",
    "        }\n",
    "        \n",
    "        amount = max(5, base_amount * category_multipliers[category])\n",
    "        \n",
    "        synthetic_data.append({\n",
    "            'date': date,\n",
    "            'amount': round(amount, 2),\n",
    "            'category': category,\n",
    "            'day_of_week': date.dayofweek,\n",
    "            'month': date.month,\n",
    "            'is_weekend': date.weekday() >= 5\n",
    "        })\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_data)\n",
    "print(f\"Generated {len(synthetic_df)} synthetic transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6180d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series features for prediction\n",
    "daily_expenses = synthetic_df.groupby('date')['amount'].sum().reset_index()\n",
    "daily_expenses['day_of_week'] = daily_expenses['date'].dt.dayofweek\n",
    "daily_expenses['month'] = daily_expenses['date'].dt.month\n",
    "daily_expenses['is_weekend'] = daily_expenses['date'].dt.weekday >= 5\n",
    "\n",
    "# Create lag features\n",
    "daily_expenses['amount_lag_1'] = daily_expenses['amount'].shift(1)\n",
    "daily_expenses['amount_lag_7'] = daily_expenses['amount'].shift(7)\n",
    "daily_expenses['rolling_7_mean'] = daily_expenses['amount'].rolling(7).mean()\n",
    "\n",
    "# Remove rows with NaN values\n",
    "daily_expenses = daily_expenses.dropna()\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['day_of_week', 'month', 'is_weekend', 'amount_lag_1', 'amount_lag_7', 'rolling_7_mean']\n",
    "X = daily_expenses[feature_cols]\n",
    "y = daily_expenses['amount']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train Random Forest regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Expense Prediction Model Performance:\")\n",
    "print(f\"MAE: ${mae:.2f}\")\n",
    "print(f\"RMSE: ${rmse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb68c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.values, label='Actual', alpha=0.7)\n",
    "plt.plot(y_pred, label='Predicted', alpha=0.7)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Daily Expense ($)')\n",
    "plt.title('Actual vs Predicted Daily Expenses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Feature importance for expense prediction\n",
    "feature_importance = rf_regressor.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(feature_cols, feature_importance)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance for Expense Prediction')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Future predictions\n",
    "print(\"\\nFuture Expense Predictions (next 7 days):\")\n",
    "last_row = daily_expenses.iloc[-1]\n",
    "future_predictions = []\n",
    "\n",
    "for i in range(1, 8):\n",
    "    future_date = last_row['date'] + timedelta(days=i)\n",
    "    future_features = [\n",
    "        future_date.weekday(),  # day_of_week\n",
    "        future_date.month,      # month\n",
    "        future_date.weekday() >= 5,  # is_weekend\n",
    "        last_row['amount'],     # amount_lag_1 (simplified)\n",
    "        daily_expenses['amount'].iloc[-7:].mean(),  # amount_lag_7 (simplified)\n",
    "        daily_expenses['amount'].iloc[-7:].mean()   # rolling_7_mean (simplified)\n",
    "    ]\n",
    "    \n",
    "    future_scaled = scaler.transform([future_features])\n",
    "    prediction = rf_regressor.predict(future_scaled)[0]\n",
    "    future_predictions.append(prediction)\n",
    "    \n",
    "    print(f\"Day {i} ({future_date.strftime('%Y-%m-%d')}): ${prediction:.2f}\")\n",
    "\n",
    "print(f\"\\nTotal predicted expenses for next 7 days: ${sum(future_predictions):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec51b21",
   "metadata": {},
   "source": [
    "## 8. Summary and Recommendations\n",
    "\n",
    "This analysis provides key insights for the SmartSpend application:\n",
    "\n",
    "### Key Findings:\n",
    "1. **Spending Patterns**: Clear patterns emerge based on day of week and category\n",
    "2. **Category Classification**: Machine learning can accurately categorize transactions\n",
    "3. **Anomaly Detection**: Both statistical and ML methods can identify unusual transactions\n",
    "4. **Expense Prediction**: Historical data can be used to predict future spending\n",
    "\n",
    "### Model Performance:\n",
    "- **Category Classifier**: Achieved high accuracy in categorizing transactions\n",
    "- **Anomaly Detector**: Successfully identified outliers using multiple methods\n",
    "- **Expense Predictor**: Reasonable performance in predicting daily expenses\n",
    "\n",
    "### Recommendations for Implementation:\n",
    "1. **Data Collection**: Ensure consistent transaction data collection\n",
    "2. **Model Updates**: Regularly retrain models with new data\n",
    "3. **User Feedback**: Implement feedback loops to improve model accuracy\n",
    "4. **Real-time Processing**: Deploy models for real-time transaction processing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
